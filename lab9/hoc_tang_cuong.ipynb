{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a805a2f8",
   "metadata": {},
   "source": [
    "\n",
    "# Thực hành 20-24: Tổng hợp các bài thực hành Reinforcement Learning \n",
    "**Các bài thực hành:** THỰC HÀNH 20 → 24  \n",
    "\n",
    "Notebook này tổng hợp các ví dụ tìm đường (DFS, BFS) và Q‑learning trên lưới, kèm giải thích và nhận xét.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f66497",
   "metadata": {},
   "source": [
    "## THỰC HÀNH 20 – DFS tìm đường trong mê cung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc89b27",
   "metadata": {},
   "source": [
    "\n",
    "### Giải thích thuật toán  \n",
    "Thuật toán tìm kiếm theo chiều sâu **(DFS - Depth-First Search)** được sử dụng để tìm đường đi trong mê cung 6x6. DFS xuất phát từ ô bắt đầu, cố gắng đi sâu theo một nhánh cho đến khi gặp ngõ cụt hoặc đến đích thì quay lui. Cách tiếp cận này sử dụng ngăn xếp (hoặc đệ quy) để duyệt các ô liền kề. Trong mê cung, DFS có thể tìm thấy một đường đi từ điểm đầu đến điểm cuối, nhưng không đảm bảo đó là đường đi ngắn nhất. Thuật toán sẽ thăm các ô (trạng thái) theo thứ tự ưu tiên đã định (ví dụ: lên, xuống, trái, phải) cho đến khi tìm được mục tiêu hoặc không còn nước đi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# THỰC HÀNH 20 – DFS on 6×6 maze\n",
    "maze_size = 6\n",
    "obstacles = [(0,1),(1,1),(4,1),(4,2),(3,2),(4,3),(3,3),(0,4),(3,5)]\n",
    "start, goal = (0,0), (0,5)\n",
    "\n",
    "def is_valid(x, y):\n",
    "    return (0 <= x < maze_size) and (0 <= y < maze_size) and (x,y) not in obstacles\n",
    "\n",
    "def dfs(current, visited, path):\n",
    "    x, y = current\n",
    "    if current == goal:\n",
    "        path.append(current); return True\n",
    "    visited.add(current)\n",
    "    for move in [(x-1,y),(x+1,y),(x,y-1),(x,y+1)]:   # lên, xuống, trái, phải\n",
    "        if is_valid(*move) and move not in visited:\n",
    "            if dfs(move, visited, path):\n",
    "                path.append(current); return True\n",
    "    return False\n",
    "\n",
    "visited, path = set(), []\n",
    "if dfs(start, visited, path):\n",
    "    path.reverse()\n",
    "    print(\"Path found:\"); print(path)\n",
    "else:\n",
    "    print(\"No path found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583d1c1",
   "metadata": {},
   "source": [
    "\n",
    "**Nhận xét:**  Ở bài toán này, mê cung được biểu diễn dưới dạng lưới 6x6 với các ô bị chặn (chướng ngại vật) được liệt kê trong danh sách obstacles. Thuật toán DFS duyệt mê cung từ ô bắt đầu (0,0) và tìm được một đường đi đến ô mục tiêu (0,5). Kết quả in ra cho thấy trình tự các tọa độ từ điểm bắt đầu đến đích. Đường đi mà DFS tìm được có thể không phải đường ngắn nhất, nhưng thuật toán đã thành công trong việc tìm một lối thoát qua mê cung bằng cách đi sâu vào các nhánh trước khi quay lui khi gặp ngõ cụt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa355ee4",
   "metadata": {},
   "source": [
    "## THỰC HÀNH 21 – DFS với mê cung thay đổi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5700074d",
   "metadata": {},
   "source": [
    "### Giải thích thuật toán\n",
    "Bài thực hành này tiếp tục sử dụng thuật toán DFS để tìm đường đi, nhưng với một cấu hình mê cung (lưới 6x6) khác. Thuật toán DFS vẫn hoạt động tương tự như ở Thực hành 20: nó thăm các ô kề chưa được thăm, đi sâu cho đến khi chạm đích hoặc không còn nước đi, sau đó backtrack. Việc thay đổi vị trí các chướng ngại vật sẽ ảnh hưởng đến đường đi mà DFS tìm thấy. Tuy nhiên, do DFS ưu tiên đi theo một nhánh sâu, đường đi tìm được vẫn có thể không phải ngắn nhất. Mục tiêu của bài này là so sánh kết quả DFS trên mê cung thay đổi và nhận xét về sự khác biệt (nếu có) trong đường đi tìm được."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf23250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# THỰC HÀNH 21 – DFS với mê cung khác\n",
    "maze_size = 6\n",
    "obstacles = [(0,1),(2,2),(3,2),(4,2),(5,2),(0,3),(2,4),(5,4)]\n",
    "start, goal = (0,0), (0,5)\n",
    "\n",
    "def is_valid(x, y):\n",
    "    return (0 <= x < maze_size) and (0 <= y < maze_size) and (x,y) not in obstacles\n",
    "\n",
    "def dfs(current, visited, path):\n",
    "    x, y = current\n",
    "    if current == goal:\n",
    "        path.append(current); return True\n",
    "    visited.add(current)\n",
    "    for move in [(x-1,y),(x+1,y),(x,y-1),(x,y+1)]:\n",
    "        if is_valid(*move) and move not in visited:\n",
    "            if dfs(move, visited, path):\n",
    "                path.append(current); return True\n",
    "    return False\n",
    "\n",
    "visited, path = set(), []\n",
    "if dfs(start, visited, path):\n",
    "    path.reverse()\n",
    "    print(\"Path found:\"); print(path)\n",
    "else:\n",
    "    print(\"No path found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9032bd",
   "metadata": {},
   "source": [
    "\n",
    "**Nhận xét:** Với cấu hình mê cung mới, **DFS** vẫn tìm được đường đi từ (0,0) đến (0,5). Danh sách các tọa độ được in ra cho thấy lộ trình cụ thể mà thuật toán đã đi qua. So sánh với Thực hành 20, đường đi trong trường hợp này dài hơn và vòng vèo hơn do cách bố trí chướng ngại vật khác biệt. Điều này minh họa rằng **DFS** sẽ tìm một đường đi (nếu tồn tại) tùy theo thứ tự duyệt, nhưng đường đi đó chưa chắc là ngắn nhất. Nếu sử dụng thuật toán tìm kiếm theo chiều rộng **(BFS)** trên cùng mê cung, ta có thể tìm được đường đi ngắn hơn (nếu tồn tại), vì **BFS** đảm bảo tìm đường đi ngắn nhất tính theo số bước. Tuy nhiên, trong ví dụ này, cả hai lần chạy DFS đều tìm được các đường đi hợp lệ, chứng tỏ khả năng của DFS trong việc khám phá không gian trạng thái để tìm mục tiêu, dù phải đánh đổi về tối ưu độ dài đường đi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddf134",
   "metadata": {},
   "source": [
    "## THỰC HÀNH 22 – BFS tìm đường ngắn nhất trên lưới 5×5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba5ee3",
   "metadata": {},
   "source": [
    "Giải thích thuật toán: Thuật toán tìm kiếm theo chiều rộng **(BFS - Breadth-First Search)** được sử dụng trong bài này để tìm đường đi ngắn nhất trên một lưới với các ô trống và chướng ngại vật. BFS khám phá các vị trí theo từng lớp (từng độ sâu) xuất phát từ điểm bắt đầu, đảm bảo rằng khi đạt đến điểm đích lần đầu tiên, ta có được đường đi tối thiểu về số bước. Cấu trúc dữ liệu hàng đợi (queue) được sử dụng để lưu trữ các vị trí sẽ thăm tiếp theo. Mỗi bước, BFS lấy ra một vị trí trong hàng đợi và thêm các ô kề (lên, xuống, trái, phải) chưa thăm và không phải chướng ngại vật vào cuối hàng đợi. Quá trình này lặp lại cho đến khi tìm thấy mục tiêu hoặc không còn ô nào để thăm. BFS đảm bảo tìm được đường đi ngắn nhất nếu tồn tại đường đi trong đồ thị không trọng số (ở đây là lưới ô vuông)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "MOVES = [(-1,0),(1,0),(0,-1),(0,1)]\n",
    "\n",
    "class FSSP_BFS:\n",
    "    def __init__(self, grid, start, goal):\n",
    "        self.grid, self.start, self.goal = grid, start, goal\n",
    "        self.rows, self.cols = len(grid), len(grid[0])\n",
    "    def is_valid(self, pos):\n",
    "        r,c = pos\n",
    "        return 0<=r<self.rows and 0<=c<self.cols and self.grid[r][c]==0\n",
    "    def bfs(self):\n",
    "        q = deque([(self.start,[self.start])]); visited={self.start}\n",
    "        while q:\n",
    "            cur,path = q.popleft()\n",
    "            if cur==self.goal: return path\n",
    "            for dr,dc in MOVES:\n",
    "                nxt=(cur[0]+dr,cur[1]+dc)\n",
    "                if self.is_valid(nxt) and nxt not in visited:\n",
    "                    visited.add(nxt); q.append((nxt,path+[nxt]))\n",
    "        return None\n",
    "    def visualize(self,path):\n",
    "        g=np.array(self.grid); fig,ax=plt.subplots(figsize=(6,6))\n",
    "        ax.imshow(g,cmap='Greys',alpha=0.8)\n",
    "        ax.text(self.start[1],self.start[0],'Start',color='green',ha='center',va='center',fontweight='bold')\n",
    "        ax.text(self.goal[1],self.goal[0],'Goal',color='red',ha='center',va='center',fontweight='bold')\n",
    "        if path is not None:\n",
    "            p=np.array(path); ax.plot(p[:,1],p[:,0],color='blue',marker='o',markersize=8,markerfacecolor='yellow')\n",
    "        ax.set_xticks(np.arange(self.cols)); ax.set_yticks(np.arange(self.rows))\n",
    "        ax.grid(which='both',color='black'); plt.show()\n",
    "\n",
    "grid=[[0,0,0,1,0],\n",
    "      [0,1,0,1,0],\n",
    "      [0,1,0,0,0],\n",
    "      [0,1,0,0,1],\n",
    "      [1,1,0,0,0]]\n",
    "planner=FSSP_BFS(grid,(0,0),(4,4))\n",
    "path=planner.bfs()\n",
    "print(\"Path:\",path)\n",
    "planner.visualize(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a38c7b",
   "metadata": {},
   "source": [
    "Nhận xét: Lưới 5x5 ở trên được biểu diễn dưới dạng ma trận nhị phân, trong đó 0 là ô trống và 1 là ô có chướng ngại vật. Điểm bắt đầu là (0,0) (góc trên bên trái) và điểm đích là (4,4) (góc dưới bên phải). Thuật toán BFS duyệt cấp độ từng ô một nên tìm được đường đi ngắn nhất đến đích (nếu tồn tại). Kết quả Path found in ra danh sách các tọa độ từ Start đến Goal. Trong trường hợp này, BFS đã tìm được đường đi và trực quan hóa nó trên hình ảnh: các ô màu đen biểu thị chướng ngại vật, đường đi được vẽ màu xanh dương đi qua các ô trống, bắt đầu từ Start màu xanh lá đến Goal màu đỏ. Ta có thể thấy BFS đã tránh các chướng ngại vật (các ô có giá trị 1) và tìm lộ trình đi vòng qua chúng để tới đích một cách tối ưu nhất (ít bước nhất)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e4ca7",
   "metadata": {},
   "source": [
    "## THỰC HÀNH 23 – BFS trên lưới 6×6 nhiều chướng ngại vật"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288dc400",
   "metadata": {},
   "source": [
    "Giải thích thuật toán: Bài này tiếp tục ứng dụng thuật toán BFS để tìm đường trên một lưới lớn hơn (6x6) với bố trí chướng ngại vật phức tạp hơn. Thuật toán BFS không thay đổi – vẫn sử dụng hàng đợi để lần lượt thăm các ô lân cận ở mức độ hiện tại trước khi sang mức tiếp theo. Với lưới 6x6 và nhiều chướng ngại vật hơn, BFS vẫn đảm bảo tìm được đường đi ngắn nhất nếu có. Bài toán minh họa ưu điểm của BFS so với DFS trong môi trường có nhiều vật cản: BFS sẽ tìm được đường đi tối ưu hoặc xác nhận không có đường nếu chướng ngại vật chắn hết lối."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BFS tương tự TH22 nhưng với lưới 6×6\n",
    "grid=[[0,0,0,1,0,0],\n",
    "      [1,1,0,0,0,0],\n",
    "      [1,1,0,0,0,0],\n",
    "      [1,1,0,0,0,0],\n",
    "      [1,1,0,0,0,0],\n",
    "      [0,0,1,0,0,0]]\n",
    "planner=FSSP_BFS(grid,(0,0),(5,5))\n",
    "path=planner.bfs()\n",
    "print(\"Path:\",path)\n",
    "planner.visualize(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e9764",
   "metadata": {},
   "source": [
    "Nhận xét: Trong lưới 6x6 này, các chướng ngại vật được bố trí thành một khối lớn bên trái (cột 0 và 1 từ hàng 1 đến hàng 4 đều là 1) và một vách ngăn dọc ở cột 2 (ô (5,2) là 1, tạo thành trở ngại để đi thẳng xuống dưới). Điểm bắt đầu (0,0) ở góc trên trái, điểm đích (5,5) ở góc dưới phải. Thuật toán BFS đã tìm thấy đường đi ngắn nhất vòng qua các chướng ngại vật: kết quả in ra là Path found với chuỗi tọa độ từ (0,0) đến (5,5). Quan sát đường đi trên hình vẽ cho thấy tác nhân phải đi sang phải trên hàng trên cùng, sau đó đi xuống vòng qua các chướng ngại vật (đi dọc cột 2 và chuyển hướng khi gặp vật cản ở ô (5,2)), rồi đi sang phải để đến đích. BFS đảm bảo rằng đây là đường đi ít bước nhất. Nếu thử dùng DFS trên cùng lưới này, có thể thuật toán sẽ đi lòng vòng hơn hoặc thậm chí duyệt lâu hơn mới tìm thấy đích, trong khi BFS tìm rất hiệu quả nhờ duyệt theo từng lớp gần dần mục tiêu.\n",
    "\n",
    "Q-learning sau đó cập nhật giá trị Q của trạng thái và hành động vừa chọn theo công thức: \n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    "←\n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    "+\n",
    "𝛼\n",
    "(\n",
    "𝑟\n",
    "+\n",
    "𝛾\n",
    "max\n",
    "⁡\n",
    "𝑎\n",
    "′\n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    "′\n",
    ",\n",
    "𝑎\n",
    "′\n",
    ")\n",
    "−\n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    ")\n",
    "Q(s,a)←Q(s,a)+α(r+γmax \n",
    "a \n",
    "′\n",
    " \n",
    "​\n",
    " Q(s \n",
    "′\n",
    " ,a \n",
    "′\n",
    " )−Q(s,a)) Trong đó:\n",
    "𝛼\n",
    "α là tốc độ học (learning rate),\n",
    "𝛾\n",
    "γ là hệ số chiết khấu tương lai (discount factor),\n",
    "𝑠\n",
    "s là trạng thái hiện tại, \n",
    "𝑎\n",
    "a là hành động đã chọn,\n",
    "𝑟\n",
    "r là phần thưởng nhận được, và \n",
    "𝑠\n",
    "′\n",
    "s \n",
    "′\n",
    "  là trạng thái mới.\n",
    "Thuật toán lặp lại quá trình trên cho đến khi kết thúc episode (đến được goal hoặc đạt điều kiện dừng). Sau nhiều epoch, bảng Q sẽ dần dần tiệm cận giá trị tối ưu, trong đó \n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    "Q(s,a) phản ánh \"giá trị\" của hành động \n",
    "𝑎\n",
    "a tại trạng thái \n",
    "𝑠\n",
    "s (tức là phần thưởng kỳ vọng nếu thực hiện hành động đó và tiếp tục theo chính sách tối ưu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc489e5",
   "metadata": {},
   "source": [
    "## THỰC HÀNH 24 – Q‑learning trên lưới 4×4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166c25a",
   "metadata": {},
   "source": [
    "Giải thích thuật toán: Thuật toán Q-learning thuộc lĩnh vực Reinforcement Learning (học tăng cường) được sử dụng để tìm chính sách tối ưu trong môi trường gồm 16 trạng thái (có thể hình dung là các ô của một lưới 4x4). Mỗi trạng thái có 4 hành động có thể (ví dụ: đi lên, xuống, trái, phải). Trong bài này, trạng thái đích (goal_state) là 15 (giả sử tương ứng ô cuối cùng). Thuật toán Q-learning sẽ cập nhật bảng Q (Q-table) kích thước 16x4 thông qua tương tác thử nghiệm ngẫu nhiên."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f512d2",
   "metadata": {},
   "source": [
    "Cụ thể, agent sẽ trải qua nhiều episode (vòng thử nghiệm, ở đây gọi là epoch), mỗi episode bắt đầu từ một trạng thái ngẫu nhiên. Tại mỗi bước trong môi trường, agent chọn hành động theo chính sách epsilon-greedy: với xác suất exploration_prob (epsilon) thì chọn hành động ngẫu nhiên (khám phá), còn lại thì chọn hành động có giá trị Q cao nhất hiện tại (khai thác). Sau khi thực hiện hành động, môi trường sẽ chuyển agent sang trạng thái kế tiếp và trả về phần thưởng (reward). Trong mô hình đơn giản này, cách chuyển trạng thái được định nghĩa là next_state = (current_state + 1) % n_states (tức là agent luôn chuyển sang trạng thái kế tiếp tuần hoàn, đây là mô phỏng đơn giản cho mục đích minh họa) và phần thưởng reward nhận được bằng 1 nếu trạng thái mới là trạng thái đích, ngược lại 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d703315",
   "metadata": {},
   "source": [
    "Q-learning sau đó cập nhật giá trị Q của trạng thái và hành động vừa chọn theo công thức: \n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    "←\n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    "+\n",
    "𝛼\n",
    "(\n",
    "𝑟\n",
    "+\n",
    "𝛾\n",
    "max\n",
    "⁡\n",
    "𝑎\n",
    "′\n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    "′\n",
    ",\n",
    "𝑎\n",
    "′\n",
    ")\n",
    "−\n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    ")\n",
    "Q(s,a)←Q(s,a)+α(r+γmax \n",
    "a \n",
    "′\n",
    " \n",
    "​\n",
    " Q(s \n",
    "′\n",
    " ,a \n",
    "′\n",
    " )−Q(s,a)) Trong đó:\n",
    "𝛼\n",
    "α là tốc độ học (learning rate),\n",
    "𝛾\n",
    "γ là hệ số chiết khấu tương lai (discount factor),\n",
    "𝑠\n",
    "s là trạng thái hiện tại, \n",
    "𝑎\n",
    "a là hành động đã chọn,\n",
    "𝑟\n",
    "r là phần thưởng nhận được, và \n",
    "𝑠\n",
    "′\n",
    "s \n",
    "′\n",
    "  là trạng thái mới.\n",
    "Thuật toán lặp lại quá trình trên cho đến khi kết thúc episode (đến được goal hoặc đạt điều kiện dừng). Sau nhiều epoch, bảng Q sẽ dần dần tiệm cận giá trị tối ưu, trong đó \n",
    "𝑄\n",
    "(\n",
    "𝑠\n",
    ",\n",
    "𝑎\n",
    ")\n",
    "Q(s,a) phản ánh \"giá trị\" của hành động \n",
    "𝑎\n",
    "a tại trạng thái \n",
    "𝑠\n",
    "s (tức là phần thưởng kỳ vọng nếu thực hiện hành động đó và tiếp tục theo chính sách tối ưu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc02dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "n_states, n_actions, goal_state = 16, 4, 15\n",
    "Q=np.zeros((n_states,n_actions))\n",
    "alpha,gamma,eps,epochs = 0.1,0.9,0.2,1000\n",
    "\n",
    "for _ in range(epochs):\n",
    "    s=np.random.randint(n_states)\n",
    "    while s!=goal_state:\n",
    "        a=np.random.randint(n_actions) if np.random.rand()<eps else np.argmax(Q[s])\n",
    "        s2=(s+1)%n_states\n",
    "        r=1 if s2==goal_state else 0\n",
    "        Q[s,a]+=alpha*(r+gamma*np.max(Q[s2])-Q[s,a])\n",
    "        s=s2\n",
    "\n",
    "grid=np.max(Q,axis=1).reshape(4,4)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(grid,cmap='coolwarm',interpolation='nearest')\n",
    "plt.colorbar(label='Q-value'); plt.title('Learned Q-values')\n",
    "plt.xticks(range(4)); plt.yticks(range(4)); plt.gca().invert_yaxis()\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.text(j,i,f'{grid[i,j]:.2f}',ha='center',va='center')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learned Q-table:\\n\",Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5670b",
   "metadata": {},
   "source": [
    "Nhận xét: Biểu đồ nhiệt ở trên mô tả giá trị Q tối đa (tương ứng với hành động tốt nhất) tại mỗi trạng thái sau khi thuật toán Q-learning đã chạy 1000 tập. Ta có thể thấy các giá trị Q gần trạng thái mục tiêu tăng dần lên. Cụ thể, những ô ở gần trạng thái đích (mã trạng thái 15) có màu đỏ đậm hơn và các số liệu lớn hơn, trong khi những ô xa đích hơn thì giá trị nhỏ hơn (màu xanh lam). Điều này phản ánh việc Q-learning đã học được rằng càng tiến gần đến mục tiêu thì phần thưởng tương lai càng cao, do đó giá trị Q của những trạng thái trên đường đến đích tăng dần. Nếu quan sát Q-table đã học (in ra ở dưới cùng), ta thấy ở trạng thái đích (15) các giá trị Q cho hành động dẫn đến đích (hoặc ở đích) đạt giá trị cao (ở đây thiết lập phần thưởng trực tiếp 1 tại đích, nên hành động nào đưa vào đích sẽ có giá trị xấp xỉ 1). Các trạng thái liền trước đích có giá trị Q cao thứ hai, v.v. Như vậy, Q-learning đã lan truyền phần thưởng ngược về các trạng thái phía trước, tạo nên một dốc giá trị tăng dần hướng về mục tiêu. Điều này minh họa nguyên lý học của Q-learning: những hành động và trạng thái nào nằm trên đường dẫn tới mục tiêu sẽ dần được đánh giá cao hơn thông qua quá trình cập nhật Q."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
